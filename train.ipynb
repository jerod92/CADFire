{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CADFire Training Notebook — Iterative Branch Edition\n\nDesigned for **repeated, interruptible training runs** across multiple named\ncheckpoint branches. Each phase (1–4) can be run as many times as you like;\nevery run continues from the last checkpoint in the active branch.\n\n## Branch Metaphor\nA *branch* is a named training lineage stored in `model_saves/<branch>/`. You can:\n- **create** a fresh branch to start a new experiment\n- **fork** an existing branch to diverge from any saved snapshot\n- **resume** the latest checkpoint in any branch and keep training\n- **snapshot** the current weights with a human-readable tag at any step\n\n## Predefined Branch Names\nBranches use memorable names from a curated pool so GIFs and checkpoints are easy\nto refer to. You can also supply any custom name.\n\n## Phase Design\n| Phase | What trains | Re-runnable? | When to re-run |\n|-------|-------------|-------------|----------------|\n| **1 – Tool Classifier** | text + fusion + tool head | Yes | After new tools / task prompts |\n| **2 – Semantic Cursor** | all params | Yes | After new supervised task types |\n| **3 – Teacher Forcing** | all params | Yes | After new multi-step task types |\n| **— Diagnostics** | — | Yes | Health check at any time |\n| **4 – PPO RL** | all params | Yes | Resume whenever; pause anytime |\n\n> **Tip**: If the agent learned faster than expected, stop Phase 4 early with the\n> interrupt button (■) and continue later. Progress is always saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup (run once per instance)\n!pip install -q torch numpy matplotlib pillow imageio\n\nimport sys, os\nsys.path.insert(0, os.getcwd())\nprint('Python path set. Ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Branch management\n# Run this cell once per session.  It defines all branch helpers and loads\n# (or creates) the branch registry stored in model_saves/branches.json.\n\nimport datetime, json, shutil, time\nfrom pathlib import Path\n\nSAVES_ROOT    = Path('model_saves')\nREGISTRY_FILE = SAVES_ROOT / 'branches.json'\nSAVES_ROOT.mkdir(parents=True, exist_ok=True)\n\n# Memorable auto-assign pool\n_BRANCH_POOL = [\n    'brian', 'jimmy', 'karen', 'duke', 'atlas', 'nova', 'remy', 'sage',\n    'felix', 'iris', 'coda', 'echo', 'zara', 'orion', 'pixel', 'nexus',\n    'drift', 'quill', 'ember', 'frost', 'spark', 'lyra', 'cedar', 'storm',\n]\n\ndef _load_registry():\n    if REGISTRY_FILE.exists():\n        with open(REGISTRY_FILE) as f:\n            return json.load(f)\n    return {}\n\ndef _save_registry(reg):\n    with open(REGISTRY_FILE, 'w') as f:\n        json.dump(reg, f, indent=2)\n\ndef _branch_dir(name):\n    return SAVES_ROOT / name\n\ndef _next_auto_name():\n    used = set(_load_registry().keys())\n    for n in _BRANCH_POOL:\n        if n not in used:\n            return n\n    return 'run_' + datetime.datetime.now().strftime('%Y%m%d_%H%M')\n\ndef _branch_stats(name):\n    d   = _branch_dir(name)\n    pts = sorted(d.glob('*.pt')) if d.exists() else []\n    diag = d / 'diagnostics.json'\n    best = None\n    if diag.exists():\n        try:\n            dat  = json.load(open(diag))\n            best = dat.get('best_reward')\n        except Exception:\n            pass\n    return {'n_ckpts': len(pts),\n            'size_mb': round(sum(p.stat().st_size for p in pts)/1e6, 1),\n            'best_reward': best,\n            'tags': [p.stem for p in pts]}\n\n# ---- Public API ----------------------------------------------------------\n\ndef create_branch(name=None, note=''):\n    \"\"\"Create a new training branch. name=None auto-assigns from the pool.\"\"\"\n    if name is None:\n        name = _next_auto_name()\n    reg = _load_registry()\n    if name in reg:\n        print(f\"Branch '{name}' already exists. Use switch_branch('{name}').\")\n        return name\n    _branch_dir(name).mkdir(parents=True, exist_ok=True)\n    reg[name] = {'created': datetime.datetime.now().isoformat(),\n                 'forked_from': None, 'note': note}\n    _save_registry(reg)\n    print(f\"Created branch: '{name}'  ->  {_branch_dir(name)}/\")\n    return name\n\ndef fork_branch(from_name, to_name=None, from_tag='latest', note=''):\n    \"\"\"Fork from_name/from_tag.pt into a new branch to_name.\"\"\"\n    if to_name is None:\n        to_name = _next_auto_name()\n    src = _branch_dir(from_name) / f'{from_tag}.pt'\n    if not src.exists():\n        raise FileNotFoundError(\n            f\"'{from_tag}.pt' not found in branch '{from_name}'.\\n\"\n            f\"Available: {[p.name for p in _branch_dir(from_name).glob('*.pt')]}\")\n    _branch_dir(to_name).mkdir(parents=True, exist_ok=True)\n    shutil.copy(src, _branch_dir(to_name) / 'latest.pt')\n    reg = _load_registry()\n    reg[to_name] = {'created': datetime.datetime.now().isoformat(),\n                    'forked_from': f'{from_name}/{from_tag}', 'note': note}\n    reg.setdefault(from_name, {})\n    _save_registry(reg)\n    print(f\"Forked '{from_name}/{from_tag}' -> branch '{to_name}'\")\n    return to_name\n\ndef list_branches(verbose=True):\n    \"\"\"Show all branches with checkpoint counts and best reward.\"\"\"\n    reg = _load_registry()\n    if not reg:\n        print('No branches yet. Use create_branch() to start.')\n        return []\n    if verbose:\n        print(f\"{'Branch':<12}  {'Ckpts':<6}  {'Size':<8}  {'Best Reward':<13}  Note / Forked From\")\n        print('-' * 72)\n    names = []\n    for name in sorted(reg):\n        s = _branch_stats(name)\n        br = f\"{s['best_reward']:.4f}\" if s['best_reward'] is not None else '-'\n        note = reg[name].get('note') or reg[name].get('forked_from') or ''\n        if verbose:\n            print(f\"{name:<12}  {s['n_ckpts']:<6}  {s['size_mb']:>5.1f} MB  {br:<13}  {note}\")\n        names.append(name)\n    if verbose: print()\n    return names\n\ndef switch_branch(name):\n    \"\"\"Set the active branch for this session.\"\"\"\n    global ACTIVE_BRANCH, CKPT_DIR\n    if not _branch_dir(name).exists():\n        print(f\"Branch '{name}' not found. Create it first.\")\n        return\n    ACTIVE_BRANCH = name\n    CKPT_DIR      = str(_branch_dir(name))\n    print(f\"Active branch: '{ACTIVE_BRANCH}'  ->  {CKPT_DIR}/\")\n\ndef save_snapshot(tag, branch=None):\n    \"\"\"Copy latest.pt -> <tag>.pt as a permanent labelled snapshot.\"\"\"\n    b   = branch or ACTIVE_BRANCH\n    src = _branch_dir(b) / 'latest.pt'\n    dst = _branch_dir(b) / f'{tag}.pt'\n    if not src.exists():\n        print(f\"No latest.pt in branch '{b}'.\")\n        return\n    shutil.copy(src, dst)\n    print(f\"Snapshot saved: {dst}  ({dst.stat().st_size/1e6:.1f} MB)\")\n\ndef branch_history(branch=None, last_n=20):\n    \"\"\"Print recent PPO training log entries for a branch.\"\"\"\n    b    = branch or ACTIVE_BRANCH\n    diag = _branch_dir(b) / 'diagnostics.json'\n    if not diag.exists():\n        print(f\"No diagnostics.json in branch '{b}'.\")\n        return\n    d   = json.load(open(diag))\n    log = d.get('training_log', [])\n    print(f\"Branch '{b}' | steps={d.get('total_steps',0):,} | best={d.get('best_reward', 0):.4f}\")\n    for e in log[-last_n:]:\n        print(f\"  step={e.get('step',0):>8,}  reward={e.get('avg_reward',0):.3f}\"\n              f\"  len={e.get('avg_episode_length',0):.1f}  diff={e.get('difficulty',0):.1f}\")\n\nACTIVE_BRANCH = None\nCKPT_DIR      = None\n\nprint('Branch helpers loaded:')\nprint('  create_branch([name])    - start a new training lineage')\nprint('  fork_branch(src, [dst])  - copy a snapshot into a new branch')\nprint('  switch_branch(name)      - activate a branch for training')\nprint('  list_branches()          - show all branches')\nprint('  save_snapshot(tag)       - pin weights with a label')\nprint('  branch_history([name])   - recent RL training log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pick (or create) the active branch\n# -----------------------------------------------\n# Option A: Create a fresh auto-named branch\n# name = create_branch()            # -> 'brian', 'jimmy', ...\n\n# Option B: Create with a custom name\n# name = create_branch('my_exp')\n\n# Option C: Resume an existing branch\n# switch_branch('brian')\n\n# Option D: Fork from a snapshot in another branch\n# fork_branch('brian', 'jimmy', from_tag='phase3_final')\n# switch_branch('jimmy')\n\n# ---- Default: create a new auto-named branch ----\nname = create_branch()\nswitch_branch(name)\n\nprint()\nlist_branches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Verify environment\nimport torch\nprint(f'PyTorch : {torch.__version__}')\nprint(f'CUDA    : {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU     : {torch.cuda.get_device_name(0)}')\n    print(f'Memory  : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\nfrom cadfire.tasks.registry import TaskRegistry\nTaskRegistry.discover()\nprint(f'\\nRL tasks ({TaskRegistry.count()}): {TaskRegistry.list_tasks()[:8]}...')\n\nfrom cadfire.utils.config import num_tools\nprint(f'Tools: {num_tools()}')\n\n# Vocabulary coverage check\nfrom cadfire.utils.vocab_tracker import VocabTracker\nVocabTracker().build(); VocabTracker().print_summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 — Tool Classifier Pretraining\n\n**Re-runnable.** Each execution loads the latest checkpoint in the active branch\nand continues training. Run again whenever new tools or prompt variants are added.\n\n- Trains: text encoder + fusion bridge + tool head  \n- Frozen: vision encoder, spectral encoder, cursor head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 (re-runnable)\nfrom train import run_pretrain_tool\nassert CKPT_DIR, 'Run Cell 3 first to pick a branch.'\n\nagent, history1 = run_pretrain_tool(\n    num_epochs=30,    # increase to 50 for ~95%+ accuracy\n    lr=1e-3,\n    batch_size=64,\n    checkpoint_dir=CKPT_DIR,\n)\n\nprint(f'Phase 1 accuracy : {history1[\"tool_accuracies\"][-1]:.1%}')\nprint(f'Phase 1 loss     : {history1[\"tool_losses\"][-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 eval + snapshot\nimport matplotlib.pyplot as plt\n\ndef _plot_history(h, title=''):\n    has_cursor = bool(h.get('cursor_losses'))\n    ncols = 3 if has_cursor else 2\n    fig, axes = plt.subplots(1, ncols, figsize=(5*ncols, 3))\n    axes[0].plot(h.get('tool_losses', []), color='steelblue')\n    axes[0].set_title('Tool Loss'); axes[0].set_xlabel('Epoch')\n    axes[1].plot(h.get('tool_accuracies', []), color='green')\n    axes[1].set_title('Tool Accuracy'); axes[1].set_xlabel('Epoch'); axes[1].set_ylim(0,1)\n    if has_cursor:\n        axes[2].plot(h['cursor_losses'], color='orange')\n        axes[2].set_title('Cursor Loss'); axes[2].set_xlabel('Epoch')\n    if title: fig.suptitle(title, fontsize=12)\n    plt.tight_layout(); plt.show()\n\n_plot_history(history1, title=f'Phase 1 - {ACTIVE_BRANCH}')\nsave_snapshot('phase1_final')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 — Semantic Cursor Pretraining\n\n**Re-runnable.** Run again after adding new supervised task types.\n\nTrains all parameters on 20+ task types with Gaussian cursor targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 (re-runnable)\nfrom train import run_pretrain_semantic\n\nagent, history2 = run_pretrain_semantic(\n    agent=globals().get('agent'),   # reuse if in memory\n    num_samples=20_000,\n    num_epochs=20,\n    lr=3e-4,\n    batch_size=32,\n    sigma=12.0,\n    cursor_weight=1.0,\n    num_workers=0,\n    checkpoint_dir=CKPT_DIR,\n)\nprint(f'Phase 2 accuracy    : {history2[\"tool_accuracies\"][-1]:.1%}')\nprint(f'Phase 2 cursor loss : {history2[\"cursor_losses\"][-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_history(history2, title=f'Phase 2 - {ACTIVE_BRANCH}')\nsave_snapshot('phase2_final')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 — Teacher-Forced Multi-Step Pretraining\n\n**Re-runnable.** Run again after adding new multi-step task types.\n\nOracle actions drive the environment; agent loss is computed per step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 (re-runnable)\nfrom train import run_pretrain_teacher\n\nagent, history3 = run_pretrain_teacher(\n    agent=globals().get('agent'),\n    num_trajectories=5_000,\n    num_epochs=15,\n    lr=1e-4,\n    batch_size=16,\n    polygon_ratio=0.7,\n    checkpoint_dir=CKPT_DIR,\n)\nprint(f'Phase 3 accuracy : {history3[\"tool_accuracies\"][-1]:.1%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_history(history3, title=f'Phase 3 - {ACTIVE_BRANCH}')\nsave_snapshot('phase3_final')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics — Multi-Task GIFs with Prompt Overlay\n\nGenerates animated GIFs for polygon tracing AND a variety of RL task types.\nEach frame shows the viewport, cursor heatmap, and the **current text prompt**.\n\nRun at any point — after any phase, or during a PPO pause.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics - polygon tracing (classic)\nimport torch\nfrom cadfire.training.diagnostics import generate_diagnostic_gifs\n\ndevice   = 'cuda' if torch.cuda.is_available() else 'cpu'\ndiag_dir = f'diagnostics/{ACTIVE_BRANCH}'\n\ngif_metrics = generate_diagnostic_gifs(\n    agent, output_dir=diag_dir, n_episodes=6, device=device, fps=1.5, verbose=True\n)\nprint(f'GIFs -> {diag_dir}/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics - RL task rollouts (draw / select / modify / view)\nfrom cadfire.training.diagnostics import generate_task_rollout_gifs\n\ntask_metrics = generate_task_rollout_gifs(\n    agent,\n    task_categories=['draw', 'select', 'modify', 'view'],\n    n_per_category=2,\n    output_dir=f'diagnostics/{ACTIVE_BRANCH}/rl_tasks',\n    device=device,\n    fps=1.5,\n    verbose=True,\n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 — PPO Reinforcement Learning\n\n**Re-runnable / resumable.** Always loads the latest checkpoint in the active\nbranch. Interrupt at any time with the stop button — progress is saved every\n`save_interval` steps.\n\n**Workflow tips:**\n- Watch the reward curve. If it plateaus, interrupt, re-run Phase 2/3, then resume.\n- Snapshot at milestones: `save_snapshot('rl_v2')`.\n- Fork to try hyperparameter variants: `fork_branch(ACTIVE_BRANCH, 'jimmy')`.\n- Load a specific snapshot: `fork_branch('brian', 'test', from_tag='phase3_final')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 - PPO (resumable, interruptible)\nfrom train import run_training\n\nrun_training(\n    num_steps=200_000,   # interrupt anytime; re-run to continue\n    resume=True,         # always resume from latest in the branch\n    checkpoint_dir=CKPT_DIR,\n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO training curves for the active branch\nimport matplotlib.pyplot as plt, json\nfrom pathlib import Path\n\ndiag_path = Path(CKPT_DIR) / 'diagnostics.json'\nif not diag_path.exists():\n    print('No diagnostics.json found. Run Phase 4 first.')\nelse:\n    d   = json.load(open(diag_path))\n    log = d['training_log']\n    steps   = [e['step'] for e in log]\n    rewards = [e.get('avg_reward', 0) for e in log]\n    diffs   = [e.get('difficulty', 0) for e in log]\n    entropies = [e.get('entropy', 0) for e in log]\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n    axes[0].plot(steps, rewards, color='steelblue', lw=1)\n    axes[0].set_title('Avg Reward'); axes[0].set_xlabel('Step')\n    axes[1].plot(steps, diffs, color='orange', lw=1)\n    axes[1].set_title('Curriculum Difficulty'); axes[1].set_xlabel('Step')\n    axes[2].plot(steps, entropies, color='green', lw=1)\n    axes[2].set_title('Policy Entropy'); axes[2].set_xlabel('Step')\n\n    fig.suptitle(f'Branch: {ACTIVE_BRANCH}  Best reward: {d.get(\"best_reward\",0):.4f}')\n    plt.tight_layout(); plt.show()\n    print(f'Total steps: {d.get(\"total_steps\",0):,}')\n    print(f'Best reward: {d.get(\"best_reward\", float(\"-inf\")):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reward curves across all branches\nimport matplotlib.pyplot as plt, json\nfrom pathlib import Path\n\ncolors = ['steelblue','orange','green','red','purple','brown','teal','magenta']\nfig, ax = plt.subplots(figsize=(12, 4))\n\nfor i, bname in enumerate(list_branches(verbose=False)):\n    diag = Path('model_saves') / bname / 'diagnostics.json'\n    if not diag.exists():\n        continue\n    d   = json.load(open(diag))\n    log = d.get('training_log', [])\n    if not log:\n        continue\n    steps   = [e['step'] for e in log]\n    rewards = [e.get('avg_reward', 0) for e in log]\n    ax.plot(steps, rewards, label=bname, color=colors[i % len(colors)], lw=1.2)\n\nax.set_title('Reward Comparison - All Branches')\nax.set_xlabel('Training Step'); ax.set_ylabel('Avg Reward')\nax.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint browser - all branches\nimport time\nfrom pathlib import Path\n\nprint(f\"{'Modified':<18}  {'Size':<9}  {'Branch':<12}  Tag\")\nprint('-' * 62)\nfor pt in sorted(Path('model_saves').rglob('*.pt'),\n                 key=lambda p: p.stat().st_mtime):\n    mtime  = time.strftime('%Y-%m-%d %H:%M', time.localtime(pt.stat().st_mtime))\n    size   = f'{pt.stat().st_size / 1e6:.1f} MB'\n    branch = pt.parent.name\n    print(f'{mtime:<18}  {size:<9}  {branch:<12}  {pt.stem}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific checkpoint from any branch\n# Useful for: inspecting old checkpoints, comparing agent behaviour,\n# or forking from a specific milestone.\n\nimport torch\nfrom cadfire.model.cad_agent import CADAgent\nfrom cadfire.training.checkpoint import CheckpointManager\nfrom cadfire.utils.config import load_config\n\n# Choose which branch and tag to load:\nLOAD_BRANCH = ACTIVE_BRANCH      # or any other branch name\nLOAD_TAG    = 'phase3_final'     # or 'latest', 'best', 'rl_v2', ...\n\nconfig  = load_config()\nagent2  = CADAgent(config)\ndevice  = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nckpt = CheckpointManager(f'model_saves/{LOAD_BRANCH}', config)\nmeta = ckpt.load(agent2, optimizer=None, tag=LOAD_TAG, device=device)\nprint(f\"Loaded branch='{LOAD_BRANCH}' tag='{LOAD_TAG}' step={meta.get('step',0):,}\")\nagent2.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agent behavior on a specific task\nimport numpy as np, torch, matplotlib.pyplot as plt\nfrom cadfire.env.cad_env import CADEnv\nfrom cadfire.tasks.registry import TaskRegistry\nfrom cadfire.utils.config import load_config\n\nTaskRegistry.discover()\nconfig = load_config()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nenv  = CADEnv(config, task_category='draw')  # try 'select', 'modify', 'view'\nobs, info = env.reset()\nprompt = info.get('prompt', '')\nprint(f'Prompt: {prompt}')\n\nframes = []\nfor _ in range(20):\n    obs_t = {k: torch.from_numpy(v).float().unsqueeze(0).to(device)\n             if isinstance(v, np.ndarray) else\n             torch.tensor([[v]], dtype=torch.long).to(device)\n             for k, v in obs.items()}\n    with torch.no_grad():\n        act = agent.act(obs_t, deterministic=True)\n    obs, r, term, trunc, info = env.step({\n        'tool_id': act['tool_id'].item(),\n        'cursor':  act['cursor'].squeeze().cpu().numpy(),\n        'param':   act['param'].item(),\n    })\n    frames.append(obs['image'][:,:,:3].copy())\n    if term or trunc: break\n\ncols = min(6, len(frames))\nfig, axes = plt.subplots(1, cols, figsize=(3*cols, 3))\naxes = axes if cols > 1 else [axes]\nfor ax, fr in zip(axes, frames[::max(1, len(frames)//cols)]):\n    ax.imshow((fr*255).astype('uint8')); ax.axis('off')\nplt.suptitle(f'Prompt: {prompt}'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to DXF\nfrom cadfire.export.dxf_writer import DXFWriter\nfrom cadfire.engine.cad_engine import CADEngine\nfrom cadfire.utils.config import load_config\n\nconfig = load_config()\nengine = CADEngine(config)\n# (populate engine.entities first, e.g. by running a task)\n\nout_path = f'output_{ACTIVE_BRANCH}.dxf'\nDXFWriter(config).write(engine, out_path)\nprint(f'DXF written to: {out_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull latest code + continue (tool-list growth handled automatically)\nimport subprocess, importlib\n\nresult = subprocess.run(['git', 'pull', 'origin', 'main'],\n                        capture_output=True, text=True)\nprint(result.stdout.strip() or result.stderr.strip())\n\nimport cadfire.utils.config as _cfg\nimportlib.reload(_cfg)\nfrom cadfire.utils.config import num_tools\nprint(f'Tools after pull: {num_tools()}')\n\n# Re-run Phase 1 to absorb new prompts, then continue PPO\n# from train import run_pretrain_tool, run_training\n# run_pretrain_tool(num_epochs=20, checkpoint_dir=CKPT_DIR)\n# run_training(num_steps=100_000, resume=True, checkpoint_dir=CKPT_DIR)\nprint('Uncomment the lines above to retrain + continue after pull.')\n"
   ]
  }
 ]
}